{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requestsNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\miche\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp37-cp37m-win_amd64.whl (98 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Installing collected packages: idna, urllib3, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2023.11.17 charset-normalizer-3.3.2 idna-3.6 requests-2.31.0 urllib3-2.0.7\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached soupsieve-2.4.1-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.3 soupsieve-2.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\miche\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests\n",
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Definisci l'URL della pagina web da scaricare\n",
    "url = 'https://www.immobiliare.it/vendita-case/roma/?criterio=prezzo&ordine=asc'\n",
    "\n",
    "# Effettua una richiesta HTTP alla pagina web\n",
    "response = requests.get(url)\n",
    "data = {}\n",
    "# Verifica se la richiesta è andata a buon fine\n",
    "for i in range(1, 2):\n",
    "\n",
    "    url += ('&pag=' + str(i))\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Parsing del contenuto HTML utilizzando BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # print('Titolo della pagina:', soup.title.text)\n",
    "        # Trova tutti gli elementi HTML che contengono i titoli delle notizie (questo è un esempio, potrebbe variare a seconda del sito)\n",
    "        superficie = soup.find_all('li', class_='nd-list__item in-feat__item', attrs={'aria-label': \"superficie\"})\n",
    "        prices = soup.find_all('div', class_='in-reListCardPrice')\n",
    "        # Find all 'a' elements within the specified 'div' element\n",
    "        links = soup.find_all('div', class_='nd-mediaObject__content in-reListCard__content is-spaced')\n",
    "        \n",
    "        # Iterate over each link\n",
    "        for link in links:\n",
    "            a_tag = link.find('a')\n",
    "            if a_tag is not None:\n",
    "                href = a_tag.get('href')  # Get the 'href' attribute\n",
    "                title = a_tag.get('title')  # Get the 'title' attribute\n",
    "                print(f'Link: {href}, Title: {title}')\n",
    "        # Stampare i titoli delle notizie\n",
    "        for m2 in superficie:\n",
    "            print(m2.text)\n",
    "        for price in prices:\n",
    "            print(price.text)\n",
    "        # Initialize an empty dictionary to store the data\n",
    "\n",
    "        # Iterate over each link\n",
    "        # for link, m2, price in zip(links, superficie, prices):\n",
    "        #     a_tag = link.find('a')\n",
    "        #     if a_tag is not None:\n",
    "        #         href = a_tag.get('href')  # Get the 'href' attribute\n",
    "        #         title = a_tag.get('title')  # Get the 'title' attribute\n",
    "        #         int_price = int(price.text.replace(\"€\", \"\").replace(\".\", \"\").replace(\" \", \"\").replace(\"da\", \"\").replace(\",\", \"\"))\n",
    "        #         # Add the data to the dictionary\n",
    "        #         data[title] = {'Link': href,'Prezzo al m2' : (int(m2.text.replace(\"m²\", \"\"))), 'Superficie': m2.text, 'Price': price.text}\n",
    "\n",
    "        # # Print the data\n",
    "        # for title, info in data.items():\n",
    "        #     print(f'Title: {title}, Link: {info[\"Link\"]}, Prezzo al m²: {info[\"Prezzo al m2\"]}, Superficie: {info[\"Superficie\"]}, Price: {info[\"Price\"]}')\n",
    "    else:\n",
    "        print('Errore nella richiesta HTTP:', response.status_code)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
